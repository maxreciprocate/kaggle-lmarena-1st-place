## WSDM Cup - Multilingual Chatbot Arena

https://www.kaggle.com/competitions/wsdm-cup-multilingual-chatbot-arena

## Requirements

### Hardware

H100x8 (to speed up teachers training we used H100x32, however it's not required)

### Software

```
python3.12
SLURM
```

### Packages
```
deepspeed==0.15.4
accelerate==1.2.1
transformers==4.46.3
flash-attn==2.7.1.post4
```

## Reproduction 

### Preparation

Download the following datasets and put them into `data/` without renaming the files (the rest of datasets are available on huggingface):
- [dataset for the challenge](https://www.kaggle.com/competitions/wsdm-cup-multilingual-chatbot-arena/data) 
- Datasets from @nbroad ðŸ¤— ([v1, v2 and v3](https://www.kaggle.com/datasets/nbroad/wsdm-open-models-nbroad))
- [lmarena-ai/gpt-4o-mini_battles](https://huggingface.co/spaces/lmarena-ai/gpt-4o-mini_battles/)

### Training

- [Stage 1. Pretraining](./stage1)
- [Stage 2. Teachers training](./stage2)
- [Stage 3. Distillation](./stage3)

## Structure

All scripts have to be executed from the root directory

```
â”œâ”€â”€ data
â”‚Â Â  â”œâ”€â”€ train.parquet
â”‚Â Â  â””â”€â”€ ...
â”œâ”€â”€ ckpts
â”‚Â Â  â””â”€â”€ ...
â”œâ”€â”€ stage1
â”‚Â Â  â”œâ”€â”€ README.md
â”‚Â Â  â””â”€â”€ prepare_pretrain_data.py
â”œâ”€â”€ stage2
â”‚Â Â  â”œâ”€â”€ README.md
â”‚Â Â  â””â”€â”€ prepare_teacher_data.py
â”œâ”€â”€ stage3
â”‚Â Â  â”œâ”€â”€ README.md
â”‚Â Â  â”œâ”€â”€ collect_labels.py
â”‚Â Â  â”œâ”€â”€ merge_students.py
â”‚Â Â  â”œâ”€â”€ pack_student.py
â”‚Â Â  â”œâ”€â”€ prepare_student_data.py
â”‚Â Â  â””â”€â”€ prepare_synth_data.py
â”œâ”€â”€ packing
â”‚Â Â  â””â”€â”€ # https://github.com/tascj/kaggle-lmsys-chatbot-arena/tree/main/human_pref
â”œâ”€â”€ format.py
â”œâ”€â”€ label.py
â”œâ”€â”€ label.sh
â”œâ”€â”€ label.slurm
â”œâ”€â”€ launch.slurm
â”œâ”€â”€ models.py
â”œâ”€â”€ readme.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ run.sh
â””â”€â”€ train.py
```
